import dataclasses
import os
import shlex
import subprocess
import tempfile
import time
from pathlib import Path
from types import NoneType
from typing import Optional, Union

from consts import CUDA_FLAGS, ExitCode


@dataclasses.dataclass
class CompileResult:
    # fmt: off
    nvcc_found: bool    # did we find nvcc?
    nvcc_version: str   # the result of nvcc --version
    success: bool       # did it compile successfully
    command: str        # the command that was run to compile the code
    stdout: str         # standard output produced by the compiler
    stderr: str         # standard error produced by the compiler
    exit_code: int      # exit code produced by the compiler
    # fmt: on


@dataclasses.dataclass
class RunResult:
    # fmt: off
    success: bool       # did the compiled executable run successfully
    passed: bool        # did it pass all tests
    command: str        # the command that was run to compile the code
    stdout: str         # standard output produced by the compiler
    stderr: str         # standard error produced by the compiler
    exit_code: int      # exit code produced by the compiler
    duration: float     # execution time (NOT kernel duration)
    result: dict        # dictionary with the results generated by the tester
    # fmt: on


@dataclasses.dataclass
class FullResult:
    # fmt: off
    success: bool                  # did the runner (github/modal) execute successfully
    error: str                     # if not success, an error message
    compile: CompileResult | None  # results of compilation
    # results of running. There can be multiple runs in one submission, using separate
    # 'test' and 'benchmark' keys, for example
    runs: dict[str, RunResult] = dataclasses.field(default_factory=dict)
    # fmt: on


def _make_cmd(args: list[str]):
    return " ".join(map(shlex.quote, args))


def _limit_length(text: Union[NoneType, str, bytes], max_len: int = 16384):
    if text is None:
        return ""
    if isinstance(text, bytes):
        text = text.decode("utf-8")
    lines = text.split("\n")
    size = 0
    for i, line in enumerate(lines):
        size += len(line) + 1
        if size + 100 > max_len:
            lines = lines[:i] + [f"[...] {len(lines) - i} lines omitted"]
            return "\n".join(lines)
    return text


def _create_files(files: Optional[dict[str, str]]):
    """
    Create text files
    Args:
        files: A dictionary mapping file names to their contents.
    Raises:
        AssertionError, if the file is not within the current working directory.
    """
    if files is None:
        return

    for name, content in files.items():
        assert Path(name).resolve().is_relative_to(Path.cwd())
        Path(name).write_text(content)


def compile_cuda_script(  # # noqa: C901
    files: list[str],
    arch: Optional[int] = None,
    include_dirs: Optional[list[str]] = None,
    defines: Optional[dict[str, str]] = None,
    libraries: Optional[list[str]] = None,
    flags: Optional[list[str]] = None,
    verbose: bool = False,
) -> CompileResult:
    """
    Compiles a set of cuda files with nvcc.

    Args:
        files: List of files to compile.
        arch: Architecture to compile for. If None, uses `native`
        include_dirs: additional include directories to supply to nvcc
        defines: Additional defines for the preprocessor
        libraries: Additional libraries to link to
        flags: Other compiler flags
        verbose: whether to print progress or be silent
    Returns:
        A `CompileResult` that summarizes the compilation process.

    """
    if flags is None:
        flags = CUDA_FLAGS

    if include_dirs is not None:
        flags += [f"-I{d}" for d in include_dirs]
        # validate include directories
        for directory in include_dirs:
            if not Path(directory).exists():
                raise FileNotFoundError(f"Directory `{directory}` does not exist")
            elif not Path(directory).is_dir():
                raise NotADirectoryError(f"`{directory}` is not a directory")

    if libraries is not None:
        flags += [f"-l{lib}" for lib in libraries]

    if defines is not None:
        for name, value in defines.items():
            # restrict macro names to valid identifiers
            if not name.isidentifier():
                raise ValueError(f"Define key `{name}` contains invalid character")

            if value is not None:
                flags.append(f"-D{name}={value}")
            else:
                flags.append(f"-D{name}")

    for flag in flags:
        if not flag.startswith("-"):
            raise ValueError(f"Flag `{flag}` should start with a dash.")

    if verbose:
        print_ = print
    else:
        print_ = lambda *args, **kwargs: None  # noqa

    # Check CUDA is available and installed correctly
    print_("[CUDA Env Check]")
    try:
        # these check cuda compiler is also available
        nvcc = subprocess.check_output(["which", "nvcc"], encoding="utf-8").strip()
        nvcc_version = subprocess.check_output(["nvcc", "--version"], encoding="utf-8")
    except subprocess.CalledProcessError as e:
        return CompileResult(
            nvcc_found=False,
            success=False,
            nvcc_version="",
            command=_make_cmd(e.cmd),
            stdout=_limit_length(e.stdout),
            stderr=_limit_length(e.stderr),
            exit_code=e.returncode,
        )

    if arch is None:
        ARCH = "-arch=native"
    else:
        ARCH = f"-gencode=arch=compute_{arch},code=sm_{arch}"

    command = [nvcc] + flags + files + [ARCH, "-o", "eval.out"]

    print_("[Compiling]")
    try:
        compile_process = subprocess.run(command, capture_output=True, text=True, check=True)
    except subprocess.CalledProcessError as e:
        return CompileResult(
            nvcc_found=True,
            success=False,
            nvcc_version=nvcc_version,
            command=_make_cmd(e.cmd),
            stdout=_limit_length(e.stdout),
            stderr=_limit_length(e.stderr),
            exit_code=e.returncode,
        )

    return CompileResult(
        nvcc_found=True,
        success=True,
        nvcc_version=nvcc_version,
        command=_make_cmd(compile_process.args),
        stdout=_limit_length(compile_process.stdout),
        stderr=_limit_length(compile_process.stderr),
        exit_code=compile_process.returncode,
    )


def run_program(args: list[str], seed: int, timeout: int = 60) -> RunResult:
    print("[Running]")
    # set up a pipe so the tester can communicate its verdict with us
    env = os.environ.copy()
    pipe_read, pipe_write = os.pipe()
    env["POPCORN_FD"] = str(pipe_write)
    env["POPCORN_SEED"] = str(seed)

    execution_start_time = time.perf_counter()
    try:
        run_process = subprocess.run(
            args,
            capture_output=True,
            text=True,
            check=False,
            env=env,
            pass_fds=[pipe_write],
            timeout=timeout,
        )
    except subprocess.TimeoutExpired as e:
        return RunResult(
            success=False,
            passed=False,
            command=_make_cmd(e.cmd),
            stdout=_limit_length(e.stdout),
            stderr=_limit_length(e.stderr),
            exit_code=ExitCode.TIMEOUT_EXPIRED,
            duration=timeout,
            result={},
        )
    execution_end_time = time.perf_counter()

    # terminate output writing
    os.close(pipe_write)
    # and fetch pipe's content
    result = os.fdopen(pipe_read, "r").read()

    result_dict = {}
    for line in result.splitlines():
        key, _, value = line.partition(":")
        if key != "" or value != "":
            result_dict[key.strip()] = value.strip()

    return RunResult(
        success=(
            run_process.returncode == ExitCode.SUCCESS
            or run_process.returncode == ExitCode.VALIDATE_FAIL
        ),
        passed=result_dict.get("check", None) == "pass",
        command=_make_cmd(run_process.args),
        stdout=_limit_length(run_process.stdout),
        stderr=_limit_length(run_process.stderr),
        exit_code=run_process.returncode,
        duration=execution_end_time - execution_start_time,
        result=result_dict,
    )


def run_evaluation(
    call: list[str],
    mode: str,
    tests: Optional[str] = None,
    benchmarks: Optional[str] = None,
    seed: Optional[int] = None,
) -> dict[str, RunResult]:
    results = {}
    if mode == "test":
        with tempfile.NamedTemporaryFile("w") as tests_file:
            tests_file.write(tests)
            tests_file.flush()
            results["test"] = run_program(call + [mode, tests_file.name], seed=seed)
    elif mode in ["benchmark", "profile"]:
        with tempfile.NamedTemporaryFile("w") as bench_file:
            bench_file.write(benchmarks)
            bench_file.flush()
            results[mode] = run_program(call + [mode, bench_file.name], seed=seed)
    elif mode in ["private", "leaderboard"]:
        # first, run the tests
        with tempfile.NamedTemporaryFile("w") as tests_file:
            tests_file.write(tests)
            tests_file.flush()
            results["test"] = run_program(call + ["test", tests_file.name], seed=seed)

        if not results["test"].passed:
            return results

        # TODO Merge results of these runs in a nice way
        with tempfile.NamedTemporaryFile("w") as bench_file:
            bench_file.write(benchmarks)
            bench_file.flush()
            results["benchmark"] = run_program(call + ["benchmark", bench_file.name], seed=seed)

        # if they pass, run the leaderboard validation
        if results["benchmark"].passed:
            with tempfile.NamedTemporaryFile("w") as bench_file:
                bench_file.write(benchmarks)
                bench_file.flush()
                results["leaderboard"] = run_program(
                    call + ["leaderboard", bench_file.name], seed=seed
                )

    else:
        assert mode == "script"
        results["script"] = run_program(call, seed=seed)

    return results


def run_cuda_script(  # # noqa: C901
    sources: dict[str, str],
    headers: Optional[dict[str, str]] = None,
    arch: Optional[int] = None,
    defines: Optional[dict[str, str]] = None,
    include_dirs: Optional[list[str]] = None,
    libraries: Optional[list[str]] = None,
    flags: Optional[list[str]] = None,
    mode: str = "script",
    tests: str = None,
    benchmarks: str = None,
    seed: int = 42,
) -> tuple[CompileResult, dict[str, RunResult]]:
    """
    Executes the provided CUDA kernel in an isolated environment

    Args:
        sources: The source files to compile. Mapping file name to content.
        headers: Additional header files to create for the compile run.
            Mapping of file name to file contents. These files will _not_ be added to the
            compile command.
        arch: The arch code for the compute/sm versions. If None, native arch is used.
        include_dirs: Additional include directories, e.g., for thunderkittens/cutlass etc
        defines: Preprocessor defines
        libraries: Additional libraries to link to
        flags: Additional flags to give to the compiler
        seed: Random seed to initialize the RNG for testing

    Returns:
        tuple[CompileResult, RunResult]: CUDA compile/eval result information
    """
    try:
        # Write submission files to directory
        _create_files(sources)
        _create_files(headers)

        compile_result = compile_cuda_script(
            files=list(sources.keys()),
            arch=arch,
            include_dirs=include_dirs,
            defines=defines,
            libraries=libraries,
            flags=flags,
            verbose=True,
        )

        if not compile_result.success:
            return compile_result, {}

    # cleaning up all source files _before_ we let the user code run, just in
    # case there's something in there that the user isn't supposed to snoop
    finally:
        tmp_files = list(sources.keys()) + list((headers or {}).keys())
        for f in tmp_files:
            if os.path.exists(f):
                os.remove(f)

    run_result = run_evaluation(["./eval.out"], mode, tests, benchmarks, seed)
    return compile_result, run_result


def run_pytorch_script(  # noqa: C901
    sources: dict[str, str],
    main: str,
    mode: str = None,
    tests: str = None,
    benchmarks: str = None,
    seed: int = 42,
) -> dict[str, RunResult]:
    """
    Executes the provided PyTorch GPU kernel in an isolated environment

    Args:
        sources: Files to generate
        main: Which file to run. Must be one of the keys in sources.
        seed: Random seed to initialize the RNG for testing

    Returns:
        RunResult
    """
    try:
        assert main in sources.keys()

        # Write submission files to directory
        _create_files(sources)

        return run_evaluation(["python", main], mode, tests, benchmarks, seed)

    finally:
        for f in sources.keys():
            if os.path.exists(f):
                os.remove(f)


def build_test_string(tests: list[dict]):
    as_str = ""
    for test in tests:
        kvs = []
        for k, v in test.items():
            kvs.append(f"{k}: {v}")
        as_str += "; ".join(kvs) + "\n"
    return as_str


def run_config(config: dict):
    if config["lang"] == "py":
        run_result = run_pytorch_script(
            sources=config["sources"],
            main=config["main"],
            tests=build_test_string(config.get("tests", [])),
            benchmarks=build_test_string(config.get("benchmarks", [])),
            mode=config.get("mode", "test"),
        )
        return FullResult(success=True, error="", compile=None, runs=run_result)
    elif config["lang"] == "cu":
        comp, run = run_cuda_script(
            sources=config["sources"],
            headers=config.get("headers", {}),
            arch=config.get("arch", None),
            defines=config.get("defines", {}),
            include_dirs=config.get("include_dirs", []),
            libraries=config.get("libraries", []),
            flags=CUDA_FLAGS,
            tests=build_test_string(config.get("tests", [])),
            benchmarks=build_test_string(config.get("benchmarks", [])),
            mode=config.get("mode", "test"),
        )
        return FullResult(success=True, error="", compile=comp, runs=run)
    else:
        raise ValueError(f"Invalid language {config['lang']}")
